# -*- coding: utf-8 -*-
"""MLSP_FINAL_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T5QcMhaAKyv0pDFTMUDJ4zxRonPWcyUO

#### dataset preparation
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Upload your CSV file from local system
from google.colab import files
uploaded = files.upload()

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/mlsp dataset/emg_all_features_labeled.csv", header=None)

# Shuffle the dataset
df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)

# Separate features and labels
X = df.iloc[:, :-1].values  # Columns 0 to 79 (features)
y = df.iloc[:, -1].values   # Column 80 (labels)

# Split into Train (70%) and Temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

# Split Temp into Validation (15%) and Test (15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

# Confirm shapes
print(f"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}")

# Optionally save splits for reuse
np.savez("emg_splits.npz",
         X_train=X_train, y_train=y_train,
         X_val=X_val, y_val=y_val,
         X_test=X_test, y_test=y_test)

# Download the saved splits (optional)
files.download("emg_splits.npz")

"""#### now we will use different combinations of extracted feature to perform classification tasks"""

import numpy as np
from sklearn.preprocessing import StandardScaler

# Load splits
data = np.load("emg_splits.npz")
X_train, y_train = data["X_train"], data["y_train"]
X_val, y_val     = data["X_val"], data["y_val"]
X_test, y_test   = data["X_test"], data["y_test"]

# Define feature names and index ranges
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

# Each feature occupies 8 consecutive columns
feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

# Function to extract selected feature subsets
def get_feature_subset(X, selected_features):
    indices = []
    for f in selected_features:
        indices.extend(feature_indices[f])
    return X[:, indices]

# Function to normalize train/val/test using train stats
def normalize_datasets(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled   = scaler.transform(X_val)
    X_test_scaled  = scaler.transform(X_test)
    return X_train_scaled, X_val_scaled, X_test_scaled

## LET'S SELECT DIFFERENT COMBINATIONS OF FEATURES ##

# Load full feature sets from split
X_train_full = X_train
X_val_full   = X_val
X_test_full  = X_test

# Normalize full feature sets
X_train_full_norm, X_val_full_norm, X_test_full_norm = normalize_datasets(
    X_train_full, X_val_full, X_test_full
)



"""## Using all features combo

#### ----- NOW WE TAKE LOGISTIC REGRESSOR MODEL AND PERFORM GRID CROSS VALIDATION SEARCH
####  FOR BEST HYPERPARAMTER COMBO

#### LR
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay,accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val for GridSearch
X_train_val = np.vstack((X_train_full_norm, X_val_full_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],      # Regularization strength
    'solver': ['liblinear', 'lbfgs'], # Solvers that work for small/mid-size data
    'penalty': ['l2']                 # Only 'l2' works with lbfgs/liblinear
}

# Grid search
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train_val, y_train_val)

# Best model
best_model = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_model.predict(X_test_full_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Logistic Regression (Best Model)")
plt.grid(False)
plt.show()

"""#### RF"""

###----- USING RANDOM FOREST ------

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_full_norm, X_val_full_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_rf = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_rf.predict(X_test_full_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Random Forest (Best Model)")
plt.grid(False)
plt.show()

"""#### SVM"""

##------ SVM -----

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_full_norm, X_val_full_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid for RBF kernel
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

# Grid search
grid = GridSearchCV(
    SVC(probability=True),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_svm = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_svm.predict(X_test_full_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: SVM (Best Model)")
plt.grid(False)
plt.show()

"""#### KNN"""

#---- Now for KNN -----#

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_full_norm, X_val_full_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski']
}

# Grid search
grid = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_knn = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_knn.predict(X_test_full_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: KNN (Best Model)")
plt.grid(False)
plt.show()

"""#### MLP"""

###---- NOw let's go with MLP fitting -----

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based for Keras categorical
num_classes = 7
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

# Optional: One-hot encoding if needed (for categorical_crossentropy)
# y_train_oh = tf.keras.utils.to_categorical(y_train_keras, num_classes)
# y_val_oh   = tf.keras.utils.to_categorical(y_val_keras, num_classes)
# y_test_oh  = tf.keras.utils.to_categorical(y_test_keras, num_classes)

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(hidden_layers=[128, 64], dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_mlp(input_dim=80, hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_full_norm, y_train_keras,
        validation_data=(X_val_full_norm, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test
    test_loss, test_acc = model.evaluate(X_test_full_norm, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_full_norm).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    # Confusion matrix
    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP")
    plt.grid(False)
    plt.show()

    return model, history

# Example config
model, history = train_and_evaluate_mlp(
    hidden_layers=[128, 64,32],
    dropout_rate=0.4,
    learning_rate=0.001,
    batch_size=64,
    epochs=50
)

"""#### 2D CNN"""

###---- now move with 2D CNN ------ ###

# Reshape feature vectors into 2D "images" of shape (8, 10, 1)
def reshape_to_2d(X):
    return X.reshape((-1, 8, 10, 1))  # (samples, height, width, channels)

X_train_2d = reshape_to_2d(X_train_full_norm)
X_val_2d   = reshape_to_2d(X_val_full_norm)
X_test_2d  = reshape_to_2d(X_test_full_norm)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

def build_cnn_model(input_shape=(8, 10, 1), dropout_rate=0.3, num_classes=7):
    model = Sequential([
        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(dropout_rate),

        Conv2D(64, kernel_size=(2, 2), activation='relu'),
        BatchNormalization(),
        Flatten(),

        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(num_classes, activation='softmax')
    ])
    return model

def train_and_evaluate_cnn(dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_cnn_model(dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_2d, y_train_keras,
        validation_data=(X_val_2d, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_2d, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_2d).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: 2D CNN")
    plt.grid(False)
    plt.show()

    return model, history

cnn_model, cnn_history = train_and_evaluate_cnn(
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### sequential modelling"""

###---- now let's do it with sequentail models-----###

###----- creating sequences-----###

import numpy as np

def build_feature_sequences(X, y, time_steps=5):
    sequences = []
    labels = []
    for i in range(len(X) - time_steps + 1):
        window = X[i:i+time_steps]
        window_labels = y[i:i+time_steps]

        # Only keep sequences where all labels are the same
        if np.all(window_labels == window_labels[0]):
            sequences.append(window)
            labels.append(window_labels[0])

    return np.array(sequences), np.array(labels)

# Step 1: Load original ordered data (no shuffle)
import pandas as pd

df = pd.read_csv("/content/emg_all_features_labeled.csv", header=None)
X_full = df.iloc[:, :-1].values  # features (80)
y_full = df.iloc[:, -1].values   # labels

# Step 2: Build sequences from full ordered data
X_seq, y_seq = build_feature_sequences(X_full, y_full, time_steps=5)

# Step 3: Normalize sequences (per feature)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
num_sequences, time_steps, num_features = X_seq.shape

# Flatten for scaling: (N*T, F)
X_seq_flat = X_seq.reshape(-1, num_features)
X_seq_scaled_flat = scaler.fit_transform(X_seq_flat)
X_seq_scaled = X_seq_scaled_flat.reshape(num_sequences, time_steps, num_features)

# Step 4: Train/Val/Test split (e.g., 70/15/15)
from sklearn.model_selection import train_test_split

X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(
    X_seq_scaled, y_seq, test_size=0.30, random_state=42, stratify=y_seq
)
X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(
    X_temp_seq, y_temp_seq, test_size=0.50, random_state=42, stratify=y_temp_seq
)

# Check
print("Train seq shape:", X_train_seq.shape)
print("Val seq shape:  ", X_val_seq.shape)
print("Test seq shape: ", X_test_seq.shape)

"""#### LSTM"""

#.  now let's make RNN LSTM.

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based
y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

def build_lstm_model(input_shape, dropout_rate=0.3, num_classes=7):
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(LSTM(32))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))

    return model

def train_and_evaluate_lstm(dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])  # (time_steps, features)
    model = build_lstm_model(input_shape, dropout_rate=dropout_rate)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_seq, y_train_seq_keras,
        validation_data=(X_val_seq, y_val_seq_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_seq, y_test_seq_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_seq).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_seq_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_seq_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: LSTM on EMG Sequences")
    plt.grid(False)
    plt.show()

    return model, history

lstm_model, lstm_history = train_and_evaluate_lstm(
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### Transformer"""

### now let's do the transformer

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Ensure labels are zero-based
y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

# Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

    ff = layers.Dense(ff_dim, activation="relu")(x)
    ff = layers.Dropout(dropout)(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# Build Transformer classifier
def build_transformer_model(input_shape, num_classes=7, dropout=0.3):
    inputs = layers.Input(shape=input_shape)

    # Project input features to match the transformer hidden size
    x = layers.Dense(128)(inputs)  # 128 is the expected attention dimension

    # Transformer encoder block
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=dropout)

    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = models.Model(inputs, outputs)
    return model


# Training and evaluation
def train_and_evaluate_transformer(dropout=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])  # (time_steps, features)
    model = build_transformer_model(input_shape, dropout=dropout)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_seq, y_train_seq_keras,
        validation_data=(X_val_seq, y_val_seq_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (Transformer)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_seq, y_test_seq_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_seq).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_seq_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_seq_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: Transformer on EMG Sequences")
    plt.grid(False)
    plt.show()

    return model, history

transformer_model, transformer_history = train_and_evaluate_transformer(
    dropout=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)



"""## using different feature combo this time"""

import numpy as np
from sklearn.preprocessing import StandardScaler

# Load splits
data = np.load("emg_splits.npz")
X_train, y_train = data["X_train"], data["y_train"]
X_val, y_val     = data["X_val"], data["y_val"]
X_test, y_test   = data["X_test"], data["y_test"]

# Define feature names and index ranges
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

# Each feature occupies 8 consecutive columns
feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

# Function to extract selected feature subsets
def get_feature_subset(X, selected_features):
    indices = []
    for f in selected_features:
        indices.extend(feature_indices[f])
    return X[:, indices]

# Function to normalize train/val/test using train stats
def normalize_datasets(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled   = scaler.transform(X_val)
    X_test_scaled  = scaler.transform(X_test)
    return X_train_scaled, X_val_scaled, X_test_scaled

selected = ["rms", "mav", "wamp"]

X_train_sub = get_feature_subset(X_train, selected)
X_val_sub   = get_feature_subset(X_val, selected)
X_test_sub  = get_feature_subset(X_test, selected)

X_train_sub.shape

X_train_sub_norm, X_val_sub_norm, X_test_sub_norm = normalize_datasets(
    X_train_sub, X_val_sub, X_test_sub
)

"""#### LR"""

### using LR

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay,accuracy_score
import matplotlib.pyplot as plt
import numpy as np



# Merge train + val for GridSearch
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],      # Regularization strength
    'solver': ['liblinear', 'lbfgs'], # Solvers that work for small/mid-size data
    'penalty': ['l2']                 # Only 'l2' works with lbfgs/liblinear
}

# Grid search
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train_val, y_train_val)

# Best model
best_model = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_model.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Logistic Regression (Best Model)")
plt.grid(False)
plt.show()

# using SVM #



"""#### RF"""



from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm,))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_rf = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_rf.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Random Forest (Best Model)")
plt.grid(False)
plt.show()

"""#### SVM"""



from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid for RBF kernel
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

# Grid search
grid = GridSearchCV(
    SVC(probability=True),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_svm = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_svm.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: SVM (Best Model)")
plt.grid(False)
plt.show()

"""#### KNN"""



from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm,X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski']
}

# Grid search
grid = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_knn = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_knn.predict(X_test_sub_norm )

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: KNN (Best Model)")
plt.grid(False)
plt.show()

"""#### MLP"""



import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based for Keras categorical
num_classes = 7
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

# Optional: One-hot encoding if needed (for categorical_crossentropy)
# y_train_oh = tf.keras.utils.to_categorical(y_train_keras, num_classes)
# y_val_oh   = tf.keras.utils.to_categorical(y_val_keras, num_classes)
# y_test_oh  = tf.keras.utils.to_categorical(y_test_keras, num_classes)

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(hidden_layers=[128, 64], dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_mlp(input_dim=24, hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_sub_norm, y_train_keras,
        validation_data=(X_val_sub_norm, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test
    test_loss, test_acc = model.evaluate( X_test_sub_norm, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_sub_norm).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    # Confusion matrix
    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP")
    plt.grid(False)
    plt.show()

    return model, history



# Example config
model, history = train_and_evaluate_mlp(
    hidden_layers=[128, 64],
    dropout_rate=0.4,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### 2D CNN"""



# Reshape feature vectors into 2D "images" of shape (8, 10, 1)
def reshape_to_2d(X):
    return X.reshape((-1, 8, 3, 1))  # (samples, height, width, channels)

X_train_2d = reshape_to_2d(X_train_sub_norm)
X_val_2d   = reshape_to_2d(X_val_sub_norm)
X_test_2d  = reshape_to_2d(X_test_sub_norm)



import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

def build_cnn_model(input_shape=(8, 3, 1), dropout_rate=0.3, num_classes=7):
    model = Sequential([
        tf.keras.Input(shape=input_shape),

        Conv2D(32, kernel_size=(3, 1), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 1)),  # Only pool over height
        Dropout(dropout_rate),

        Conv2D(64, kernel_size=(2, 1), activation='relu'),
        BatchNormalization(),
        Flatten(),

        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(num_classes, activation='softmax')
    ])
    return model


def train_and_evaluate_cnn(dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_cnn_model(dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_2d, y_train_keras,
        validation_data=(X_val_2d, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_2d, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_2d).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: 2D CNN")
    plt.grid(False)
    plt.show()

    return model, history

cnn_model, cnn_history = train_and_evaluate_cnn(
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### sequential modelling"""

import numpy as np

def build_feature_sequences(X, y, time_steps=5):
    sequences = []
    labels = []
    for i in range(len(X) - time_steps + 1):
        window = X[i:i+time_steps]
        window_labels = y[i:i+time_steps]

        # Only keep sequences where all labels are the same
        if np.all(window_labels == window_labels[0]):
            sequences.append(window)
            labels.append(window_labels[0])

    return np.array(sequences), np.array(labels)

import pandas as pd
import numpy as np

# Load original dataset
df = pd.read_csv("/content/emg_all_features_labeled.csv", header=None)
X_full = df.iloc[:, :-1].values  # all features
y_full = df.iloc[:, -1].values   # labels

# Define all feature names and extract indices for selection
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

selected = ["rms", "mav", "wamp"]
selected_indices = []
for f in selected:
    selected_indices.extend(feature_indices[f])

# Subset feature matrix
X_selected = X_full[:, selected_indices]

# Build sequences (assume build_feature_sequences is already defined)
X_seq, y_seq = build_feature_sequences(X_selected, y_full, time_steps=5)

from sklearn.preprocessing import StandardScaler

num_sequences, time_steps, num_features = X_seq.shape

# Flatten → scale → reshape back
X_seq_flat = X_seq.reshape(-1, num_features)
X_seq_scaled_flat = StandardScaler().fit_transform(X_seq_flat)
X_seq_scaled = X_seq_scaled_flat.reshape(num_sequences, time_steps, num_features)

from sklearn.model_selection import train_test_split

X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(
    X_seq_scaled, y_seq, test_size=0.30, random_state=42, stratify=y_seq
)

X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(
    X_temp_seq, y_temp_seq, test_size=0.50, random_state=42, stratify=y_temp_seq
)

print("Train seq shape:", X_train_seq.shape)
print("Val seq shape:  ", X_val_seq.shape)
print("Test seq shape: ", X_test_seq.shape)

"""#### LSTM"""

# Convert labels to 0-based (0 to 6 for 7 classes)
y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def build_lstm_model(input_shape, dropout_rate=0.3, num_classes=7):
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(LSTM(32))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))

    return model

def train_and_evaluate_lstm(X_train, y_train, X_val, y_val, X_test, y_test,
                            dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_lstm_model(input_shape, dropout_rate=dropout_rate)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (LSTM)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Test evaluation
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: LSTM (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

lstm_model, lstm_history = train_and_evaluate_lstm(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### Transformer"""

y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Transformer encoder block
def transformer_encoder(inputs, head_size=64, num_heads=2, ff_dim=128, dropout=0.3):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

    ff = layers.Dense(ff_dim, activation='relu')(x)
    ff = layers.Dropout(dropout)(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# Build the Transformer model
def build_transformer_model(input_shape, dropout=0.3, num_classes=7):
    inputs = layers.Input(shape=input_shape)

    # Project input to model dimension (e.g., 128) for attention to work
    x = layers.Dense(128)(inputs)

    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=dropout)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

def train_and_evaluate_transformer(X_train, y_train, X_val, y_val, X_test, y_test,
                                   dropout=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_transformer_model(input_shape=input_shape, dropout=dropout)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (Transformer)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: Transformer (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

transformer_model, transformer_history = train_and_evaluate_transformer(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""## USING ANOTHER COMBO OF FEATURES"""



import numpy as np
from sklearn.preprocessing import StandardScaler

# Load splits
data = np.load("emg_splits.npz")
X_train, y_train = data["X_train"], data["y_train"]
X_val, y_val     = data["X_val"], data["y_val"]
X_test, y_test   = data["X_test"], data["y_test"]

# Define feature names and index ranges
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

# Each feature occupies 8 consecutive columns
feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

# Function to extract selected feature subsets
def get_feature_subset(X, selected_features):
    indices = []
    for f in selected_features:
        indices.extend(feature_indices[f])
    return X[:, indices]

# Function to normalize train/val/test using train stats
def normalize_datasets(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled   = scaler.transform(X_val)
    X_test_scaled  = scaler.transform(X_test)
    return X_train_scaled, X_val_scaled, X_test_scaled

selected = ["std", "min", "max"]

X_train_sub = get_feature_subset(X_train, selected)
X_val_sub   = get_feature_subset(X_val, selected)
X_test_sub  = get_feature_subset(X_test, selected)

X_train_sub.shape

X_train_sub_norm, X_val_sub_norm, X_test_sub_norm = normalize_datasets(
    X_train_sub, X_val_sub, X_test_sub
)

"""#### Now we will use different ML models first

#### LR
"""



from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay,accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val for GridSearch
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],      # Regularization strength
    'solver': ['liblinear', 'lbfgs'], # Solvers that work for small/mid-size data
    'penalty': ['l2']                 # Only 'l2' works with lbfgs/liblinear
}

# Grid search
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train_val, y_train_val)

# Best model
best_model = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_model.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Logistic Regression (Best Model)")
plt.grid(False)
plt.show()

"""#### RF"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm,))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_rf = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_rf.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Random Forest (Best Model)")
plt.grid(False)
plt.show()

"""#### SVM"""



from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid for RBF kernel
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

# Grid search
grid = GridSearchCV(
    SVC(probability=True),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_svm = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_svm.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: SVM (Best Model)")
plt.grid(False)
plt.show()



"""#### KNN"""



from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm,X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski']
}

# Grid search
grid = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_knn = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_knn.predict(X_test_sub_norm )

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: KNN (Best Model)")
plt.grid(False)
plt.show()

"""#### MLP"""



import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based for Keras categorical
num_classes = 7
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

# Optional: One-hot encoding if needed (for categorical_crossentropy)
# y_train_oh = tf.keras.utils.to_categorical(y_train_keras, num_classes)
# y_val_oh   = tf.keras.utils.to_categorical(y_val_keras, num_classes)
# y_test_oh  = tf.keras.utils.to_categorical(y_test_keras, num_classes)

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(hidden_layers=[128, 64], dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_mlp(input_dim=24, hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_sub_norm, y_train_keras,
        validation_data=(X_val_sub_norm, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test
    test_loss, test_acc = model.evaluate( X_test_sub_norm, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_sub_norm).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    # Confusion matrix
    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP")
    plt.grid(False)
    plt.show()

    return model, history

# Example config
model, history = train_and_evaluate_mlp(
    hidden_layers=[128, 64],
    dropout_rate=0.4,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### 2D CNN"""



# Reshape feature vectors into 2D "images" of shape (8, 10, 1)
def reshape_to_2d(X):
    return X.reshape((-1, 8, 3, 1))  # (samples, height, width, channels)

X_train_2d = reshape_to_2d(X_train_sub_norm)
X_val_2d   = reshape_to_2d(X_val_sub_norm)
X_test_2d  = reshape_to_2d(X_test_sub_norm)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

def build_cnn_model(input_shape=(8, 3, 1), dropout_rate=0.3, num_classes=7):
    model = Sequential([
        tf.keras.Input(shape=input_shape),

        Conv2D(32, kernel_size=(3, 1), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 1)),  # Only pool over height
        Dropout(dropout_rate),

        Conv2D(64, kernel_size=(2, 1), activation='relu'),
        BatchNormalization(),
        Flatten(),

        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(num_classes, activation='softmax')
    ])
    return model


def train_and_evaluate_cnn(dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_cnn_model(dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_2d, y_train_keras,
        validation_data=(X_val_2d, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_2d, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_2d).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: 2D CNN")
    plt.grid(False)
    plt.show()

    return model, history

cnn_model, cnn_history = train_and_evaluate_cnn(
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### sequential modelling"""



def build_feature_sequences(X, y, time_steps=5):
    sequences = []
    labels = []
    i = 0
    while i + time_steps <= len(X):
        window = X[i:i+time_steps]
        window_labels = y[i:i+time_steps]

        # Only keep sequences where all labels are the same
        if np.all(window_labels == window_labels[0]):
            sequences.append(window)
            labels.append(window_labels[0])

        # Move to next non-overlapping window
        i += time_steps

    return np.array(sequences), np.array(labels)

import pandas as pd
import numpy as np

# Load original dataset
df = pd.read_csv("/content/emg_all_features_labeled.csv", header=None)
X_full = df.iloc[:, :-1].values  # all features
y_full = df.iloc[:, -1].values   # labels

# Define all feature names and extract indices for selection
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

selected = ["std", "min", "max"]
selected_indices = []
for f in selected:
    selected_indices.extend(feature_indices[f])

# Subset feature matrix
X_selected = X_full[:, selected_indices]

# Build sequences (assume build_feature_sequences is already defined)
X_seq, y_seq = build_feature_sequences(X_selected, y_full, time_steps=5)

from sklearn.preprocessing import StandardScaler

num_sequences, time_steps, num_features = X_seq.shape

# Flatten → scale → reshape back
X_seq_flat = X_seq.reshape(-1, num_features)
X_seq_scaled_flat = StandardScaler().fit_transform(X_seq_flat)
X_seq_scaled = X_seq_scaled_flat.reshape(num_sequences, time_steps, num_features)

from sklearn.model_selection import train_test_split

X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(
    X_seq_scaled, y_seq, test_size=0.30, random_state=42, stratify=y_seq
)

X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(
    X_temp_seq, y_temp_seq, test_size=0.50, random_state=42, stratify=y_temp_seq
)

print("Train seq shape:", X_train_seq.shape)
print("Val seq shape:  ", X_val_seq.shape)
print("Test seq shape: ", X_test_seq.shape)

"""#### LSTM"""



# Convert labels to 0-based (0 to 6 for 7 classes)
y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def build_lstm_model(input_shape, dropout_rate=0.3, num_classes=7):
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(LSTM(32))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))

    return model

def train_and_evaluate_lstm(X_train, y_train, X_val, y_val, X_test, y_test,
                            dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_lstm_model(input_shape, dropout_rate=dropout_rate)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (LSTM)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Test evaluation
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: LSTM (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

lstm_model, lstm_history = train_and_evaluate_lstm(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### Transformer"""



y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Transformer encoder block
def transformer_encoder(inputs, head_size=64, num_heads=2, ff_dim=128, dropout=0.3):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

    ff = layers.Dense(ff_dim, activation='relu')(x)
    ff = layers.Dropout(dropout)(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# Build the Transformer model
def build_transformer_model(input_shape, dropout=0.3, num_classes=7):
    inputs = layers.Input(shape=input_shape)

    # Project input to model dimension (e.g., 128) for attention to work
    x = layers.Dense(128)(inputs)

    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=dropout)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

def train_and_evaluate_transformer(X_train, y_train, X_val, y_val, X_test, y_test,
                                   dropout=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_transformer_model(input_shape=input_shape, dropout=dropout)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (Transformer)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: Transformer (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

transformer_model, transformer_history = train_and_evaluate_transformer(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### back to MLP"""



# Flatten each subset: (N, time_steps, features) → (N*time_steps, features)
X_train_flat = X_train_seq.reshape(-1, X_train_seq.shape[2])
X_val_flat   = X_val_seq.reshape(-1, X_val_seq.shape[2])
X_test_flat  = X_test_seq.reshape(-1, X_test_seq.shape[2])

y_train_flat = np.repeat(y_train_seq, repeats=X_train_seq.shape[1]) - 1  # 0-based
y_val_flat   = np.repeat(y_val_seq,   repeats=X_val_seq.shape[1]) - 1
y_test_flat  = np.repeat(y_test_seq,  repeats=X_test_seq.shape[1]) - 1

print("X_train_flat shape:", X_test_flat.shape)  # Should be (N_train * time_steps, 24)
print("y_train_flat shape:", y_test_flat.shape)  # Should match above

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(X_train, y_train, X_val, y_val, X_test, y_test,
                           hidden_layers=[128, 64], dropout_rate=0.3,
                           learning_rate=0.001, batch_size=64, epochs=100):

    model = build_mlp(input_dim=X_train.shape[1], hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot train/val loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (MLP)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP (Flat Non-overlapping Data)")
    plt.grid(False)
    plt.show()

    return model, history

mlp_model, mlp_history = train_and_evaluate_mlp(
    X_train_flat, y_train_flat,
    X_val_flat, y_val_flat,
    X_test_flat, y_test_flat,
    hidden_layers=[128, 64],
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""## USING ANOTHER COMBO OF FEATURES"""



import numpy as np
from sklearn.preprocessing import StandardScaler

# Load splits
data = np.load("emg_splits.npz")
X_train, y_train = data["X_train"], data["y_train"]
X_val, y_val     = data["X_val"], data["y_val"]
X_test, y_test   = data["X_test"], data["y_test"]

# Define feature names and index ranges
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

# Each feature occupies 8 consecutive columns
feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

# Function to extract selected feature subsets
def get_feature_subset(X, selected_features):
    indices = []
    for f in selected_features:
        indices.extend(feature_indices[f])
    return X[:, indices]

# Function to normalize train/val/test using train stats
def normalize_datasets(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled   = scaler.transform(X_val)
    X_test_scaled  = scaler.transform(X_test)
    return X_train_scaled, X_val_scaled, X_test_scaled

selected = ["zero_cross", "aac", "afb"]

X_train_sub = get_feature_subset(X_train, selected)
X_val_sub   = get_feature_subset(X_val, selected)
X_test_sub  = get_feature_subset(X_test, selected)

X_train_sub.shape

X_train_sub_norm, X_val_sub_norm, X_test_sub_norm = normalize_datasets(
    X_train_sub, X_val_sub, X_test_sub
)

"""### ML MODEL FITTING

#### LR
"""



from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay,accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val for GridSearch
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],      # Regularization strength
    'solver': ['liblinear', 'lbfgs'], # Solvers that work for small/mid-size data
    'penalty': ['l2']                 # Only 'l2' works with lbfgs/liblinear
}

# Grid search
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train_val, y_train_val)

# Best model
best_model = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_model.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Logistic Regression (Best Model)")
plt.grid(False)
plt.show()

"""#### RF"""



from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm,))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_rf = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_rf.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Random Forest (Best Model)")
plt.grid(False)
plt.show()

"""#### SVM"""



from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid for RBF kernel
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

# Grid search
grid = GridSearchCV(
    SVC(probability=True),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_svm = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_svm.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: SVM (Best Model)")
plt.grid(False)
plt.show()

"""#### KNN"""



from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm,X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski']
}

# Grid search
grid = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_knn = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_knn.predict(X_test_sub_norm )

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: KNN (Best Model)")
plt.grid(False)
plt.show()

"""### DL MODEL

#### MLP
"""



import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based for Keras categorical
num_classes = 7
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

# Optional: One-hot encoding if needed (for categorical_crossentropy)
# y_train_oh = tf.keras.utils.to_categorical(y_train_keras, num_classes)
# y_val_oh   = tf.keras.utils.to_categorical(y_val_keras, num_classes)
# y_test_oh  = tf.keras.utils.to_categorical(y_test_keras, num_classes)

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(hidden_layers=[128, 64], dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_mlp(input_dim=24, hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_sub_norm, y_train_keras,
        validation_data=(X_val_sub_norm, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test
    test_loss, test_acc = model.evaluate( X_test_sub_norm, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_sub_norm).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    # Confusion matrix
    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP")
    plt.grid(False)
    plt.show()

    return model, history

# Example config
model, history = train_and_evaluate_mlp(
    hidden_layers=[128, 64],
    dropout_rate=0.4,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### 2D CNN"""



# Reshape feature vectors into 2D "images" of shape (8, 10, 1)
def reshape_to_2d(X):
    return X.reshape((-1, 8, 3, 1))  # (samples, height, width, channels)

X_train_2d = reshape_to_2d(X_train_sub_norm)
X_val_2d   = reshape_to_2d(X_val_sub_norm)
X_test_2d  = reshape_to_2d(X_test_sub_norm)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

def build_cnn_model(input_shape=(8, 3, 1), dropout_rate=0.3, num_classes=7):
    model = Sequential([
        tf.keras.Input(shape=input_shape),

        Conv2D(32, kernel_size=(3, 1), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 1)),  # Only pool over height
        Dropout(dropout_rate),

        Conv2D(64, kernel_size=(2, 1), activation='relu'),
        BatchNormalization(),
        Flatten(),

        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(num_classes, activation='softmax')
    ])
    return model


def train_and_evaluate_cnn(dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_cnn_model(dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_2d, y_train_keras,
        validation_data=(X_val_2d, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_2d, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_2d).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: 2D CNN")
    plt.grid(False)
    plt.show()

    return model, history

cnn_model, cnn_history = train_and_evaluate_cnn(
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### sequential modelling"""



def build_feature_sequences(X, y, time_steps=5):
    sequences = []
    labels = []
    for i in range(len(X) - time_steps + 1):
        window = X[i:i+time_steps]
        window_labels = y[i:i+time_steps]

        # Only keep sequences where all labels are the same
        if np.all(window_labels == window_labels[0]):
            sequences.append(window)
            labels.append(window_labels[0])

    return np.array(sequences), np.array(labels)

def build_feature_sequences(X, y):
    # Reshape each sample into a sequence of length 1
    sequences = X.reshape(-1, 1, X.shape[1])  # (N, 1, num_features)
    labels = y
    return sequences, labels

import pandas as pd
import numpy as np

# Load original dataset
df = pd.read_csv("/content/emg_all_features_labeled.csv", header=None)
X_full = df.iloc[:, :-1].values  # all features
y_full = df.iloc[:, -1].values   # labels

# Define all feature names and extract indices for selection
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

selected = ["zero_cross", "aac", "afb"]
selected_indices = []
for f in selected:
    selected_indices.extend(feature_indices[f])

# Subset feature matrix
X_selected = X_full[:, selected_indices]

# Build sequences (assume build_feature_sequences is already defined)
X_seq, y_seq = build_feature_sequences(X_selected, y_full)

from sklearn.preprocessing import StandardScaler

num_sequences, time_steps, num_features = X_seq.shape

# Flatten → scale → reshape back
X_seq_flat = X_seq.reshape(-1, num_features)
X_seq_scaled_flat = StandardScaler().fit_transform(X_seq_flat)
X_seq_scaled = X_seq_scaled_flat.reshape(num_sequences, time_steps, num_features)

from sklearn.model_selection import train_test_split

X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(
    X_seq_scaled, y_seq, test_size=0.30, random_state=42, stratify=y_seq
)

X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(
    X_temp_seq, y_temp_seq, test_size=0.50, random_state=42, stratify=y_temp_seq
)

print("Train seq shape:", X_train_seq.shape)
print("Val seq shape:  ", X_val_seq.shape)
print("Test seq shape: ", X_test_seq.shape)

"""#### LSTM"""



# Convert labels to 0-based (0 to 6 for 7 classes)
y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def build_lstm_model(input_shape, dropout_rate=0.3, num_classes=7):
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(LSTM(32))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))

    return model

def train_and_evaluate_lstm(X_train, y_train, X_val, y_val, X_test, y_test,
                            dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_lstm_model(input_shape, dropout_rate=dropout_rate)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (LSTM)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Test evaluation
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: LSTM (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

lstm_model, lstm_history = train_and_evaluate_lstm(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### transformer"""



y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Transformer encoder block
def transformer_encoder(inputs, head_size=64, num_heads=2, ff_dim=128, dropout=0.3):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

    ff = layers.Dense(ff_dim, activation='relu')(x)
    ff = layers.Dropout(dropout)(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# Build the Transformer model
def build_transformer_model(input_shape, dropout=0.3, num_classes=7):
    inputs = layers.Input(shape=input_shape)

    # Project input to model dimension (e.g., 128) for attention to work
    x = layers.Dense(128)(inputs)

    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=dropout)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

def train_and_evaluate_transformer(X_train, y_train, X_val, y_val, X_test, y_test,
                                   dropout=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_transformer_model(input_shape=input_shape, dropout=dropout)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (Transformer)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: Transformer (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

transformer_model, transformer_history = train_and_evaluate_transformer(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### going back to MLP"""



# Flatten each subset: (N, time_steps, features) → (N*time_steps, features)
X_train_flat = X_train_seq.reshape(-1, X_train_seq.shape[2])
X_val_flat   = X_val_seq.reshape(-1, X_val_seq.shape[2])
X_test_flat  = X_test_seq.reshape(-1, X_test_seq.shape[2])

y_train_flat = np.repeat(y_train_seq, repeats=X_train_seq.shape[1]) - 1  # 0-based
y_val_flat   = np.repeat(y_val_seq,   repeats=X_val_seq.shape[1]) - 1
y_test_flat  = np.repeat(y_test_seq,  repeats=X_test_seq.shape[1]) - 1

print("X_train_flat shape:", X_train_flat.shape)  # Should be (N_train * time_steps, 24)
print("y_train_flat shape:", y_train_flat.shape)  # Should match above

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(X_train, y_train, X_val, y_val, X_test, y_test,
                           hidden_layers=[128, 64], dropout_rate=0.3,
                           learning_rate=0.001, batch_size=64, epochs=100):

    model = build_mlp(input_dim=X_train.shape[1], hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot train/val loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (MLP)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP (Flat Non-overlapping Data)")
    plt.grid(False)
    plt.show()

    return model, history

mlp_model, mlp_history = train_and_evaluate_mlp(
    X_train_flat, y_train_flat,
    X_val_flat, y_val_flat,
    X_test_flat, y_test_flat,
    hidden_layers=[128, 64],
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)



"""## YET ANOTHER FEATURE COMBO"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Upload your CSV file from local system
from google.colab import files
uploaded = files.upload()

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/emg_all_features_labeled.csv", header=None)

# Shuffle the dataset
df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)

# Separate features and labels
X = df.iloc[:, :-1].values  # Columns 0 to 79 (features)
y = df.iloc[:, -1].values   # Column 80 (labels)

# Split into Train (70%) and Temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

# Split Temp into Validation (15%) and Test (15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

# Confirm shapes
print(f"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}")

# Optionally save splits for reuse
np.savez("emg_splits.npz",
         X_train=X_train, y_train=y_train,
         X_val=X_val, y_val=y_val,
         X_test=X_test, y_test=y_test)

# Download the saved splits (optional)
files.download("emg_splits.npz")



import numpy as np
from sklearn.preprocessing import StandardScaler

# Load splits
data = np.load("emg_splits.npz")
X_train, y_train = data["X_train"], data["y_train"]
X_val, y_val     = data["X_val"], data["y_val"]
X_test, y_test   = data["X_test"], data["y_test"]

# Define feature names and index ranges
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

# Each feature occupies 8 consecutive columns
feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

# Function to extract selected feature subsets
def get_feature_subset(X, selected_features):
    indices = []
    for f in selected_features:
        indices.extend(feature_indices[f])
    return X[:, indices]

# Function to normalize train/val/test using train stats
def normalize_datasets(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled   = scaler.transform(X_val)
    X_test_scaled  = scaler.transform(X_test)
    return X_train_scaled, X_val_scaled, X_test_scaled

selected = ["rms", "wfl", "wamp"]

X_train_sub = get_feature_subset(X_train, selected)
X_val_sub   = get_feature_subset(X_val, selected)
X_test_sub  = get_feature_subset(X_test, selected)

X_train_sub_norm, X_val_sub_norm, X_test_sub_norm = normalize_datasets(
    X_train_sub, X_val_sub, X_test_sub
)

"""### ML MODEL FITTING

#### LR
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay,accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val for GridSearch
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],      # Regularization strength
    'solver': ['liblinear', 'lbfgs'], # Solvers that work for small/mid-size data
    'penalty': ['l2']                 # Only 'l2' works with lbfgs/liblinear
}

# Grid search
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train_val, y_train_val)

# Best model
best_model = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_model.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Logistic Regression (Best Model)")
plt.grid(False)
plt.show()

"""#### RF"""



from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm,))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_rf = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_rf.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Random Forest (Best Model)")
plt.grid(False)
plt.show()

"""#### SVM"""



from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid for RBF kernel
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

# Grid search
grid = GridSearchCV(
    SVC(probability=True),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_svm = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_svm.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: SVM (Best Model)")
plt.grid(False)
plt.show()

"""#### KNN"""



from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm,X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski']
}

# Grid search
grid = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_knn = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_knn.predict(X_test_sub_norm )

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: KNN (Best Model)")
plt.grid(False)
plt.show()





"""#### MLP"""



import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based for Keras categorical
num_classes = 7
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

# Optional: One-hot encoding if needed (for categorical_crossentropy)
# y_train_oh = tf.keras.utils.to_categorical(y_train_keras, num_classes)
# y_val_oh   = tf.keras.utils.to_categorical(y_val_keras, num_classes)
# y_test_oh  = tf.keras.utils.to_categorical(y_test_keras, num_classes)

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(hidden_layers=[128, 64], dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_mlp(input_dim=24, hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_sub_norm, y_train_keras,
        validation_data=(X_val_sub_norm, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test
    test_loss, test_acc = model.evaluate( X_test_sub_norm, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_sub_norm).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    # Confusion matrix
    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP")
    plt.grid(False)
    plt.show()

    return model, history

# Example config
model, history = train_and_evaluate_mlp(
    hidden_layers=[128, 64],
    dropout_rate=0.4,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### 2D CNN"""



# Reshape feature vectors into 2D "images" of shape (8, 10, 1)
def reshape_to_2d(X):
    return X.reshape((-1, 8, 3, 1))  # (samples, height, width, channels)

X_train_2d = reshape_to_2d(X_train_sub_norm)
X_val_2d   = reshape_to_2d(X_val_sub_norm)
X_test_2d  = reshape_to_2d(X_test_sub_norm)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

def build_cnn_model(input_shape=(8, 3, 1), dropout_rate=0.3, num_classes=7):
    model = Sequential([
        tf.keras.Input(shape=input_shape),

        Conv2D(32, kernel_size=(3, 1), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 1)),  # Only pool over height
        Dropout(dropout_rate),

        Conv2D(64, kernel_size=(2, 1), activation='relu'),
        BatchNormalization(),
        Flatten(),

        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(num_classes, activation='softmax')
    ])
    return model


def train_and_evaluate_cnn(dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_cnn_model(dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_2d, y_train_keras,
        validation_data=(X_val_2d, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_2d, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_2d).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: 2D CNN")
    plt.grid(False)
    plt.show()

    return model, history

cnn_model, cnn_history = train_and_evaluate_cnn(
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### sequential modelling"""



import numpy as np

def build_feature_sequences(X, y, time_steps=5):
    sequences = []
    labels = []
    for i in range(len(X) - time_steps + 1):
        window = X[i:i+time_steps]
        window_labels = y[i:i+time_steps]

        # Only keep sequences where all labels are the same
        if np.all(window_labels == window_labels[0]):
            sequences.append(window)
            labels.append(window_labels[0])

    return np.array(sequences), np.array(labels)

import pandas as pd
import numpy as np

# Load original dataset
df = pd.read_csv("/content/emg_all_features_labeled.csv", header=None)
X_full = df.iloc[:, :-1].values  # all features
y_full = df.iloc[:, -1].values   # labels

# Define all feature names and extract indices for selection
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

selected = ["rms", "wfl", "wamp"]
selected_indices = []
for f in selected:
    selected_indices.extend(feature_indices[f])

# Subset feature matrix
X_selected = X_full[:, selected_indices]

# Build sequences (assume build_feature_sequences is already defined)
X_seq, y_seq = build_feature_sequences(X_selected, y_full, time_steps=5)

from sklearn.preprocessing import StandardScaler

num_sequences, time_steps, num_features = X_seq.shape

# Flatten → scale → reshape back
X_seq_flat = X_seq.reshape(-1, num_features)
X_seq_scaled_flat = StandardScaler().fit_transform(X_seq_flat)
X_seq_scaled = X_seq_scaled_flat.reshape(num_sequences, time_steps, num_features)

from sklearn.model_selection import train_test_split

X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(
    X_seq_scaled, y_seq, test_size=0.30, random_state=42, stratify=y_seq
)

X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(
    X_temp_seq, y_temp_seq, test_size=0.50, random_state=42, stratify=y_temp_seq
)

print("Train seq shape:", X_train_seq.shape)
print("Val seq shape:  ", X_val_seq.shape)
print("Test seq shape: ", X_test_seq.shape)

"""#### LSTM"""



# Convert labels to 0-based (0 to 6 for 7 classes)
y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def build_lstm_model(input_shape, dropout_rate=0.3, num_classes=7):
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(LSTM(32))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))

    return model

def train_and_evaluate_lstm(X_train, y_train, X_val, y_val, X_test, y_test,
                            dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_lstm_model(input_shape, dropout_rate=dropout_rate)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (LSTM)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Test evaluation
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: LSTM (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

lstm_model, lstm_history = train_and_evaluate_lstm(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### transformer"""



y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Transformer encoder block
def transformer_encoder(inputs, head_size=64, num_heads=2, ff_dim=128, dropout=0.3):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

    ff = layers.Dense(ff_dim, activation='relu')(x)
    ff = layers.Dropout(dropout)(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# Build the Transformer model
def build_transformer_model(input_shape, dropout=0.3, num_classes=7):
    inputs = layers.Input(shape=input_shape)

    # Project input to model dimension (e.g., 128) for attention to work
    x = layers.Dense(128)(inputs)

    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=dropout)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

def train_and_evaluate_transformer(X_train, y_train, X_val, y_val, X_test, y_test,
                                   dropout=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_transformer_model(input_shape=input_shape, dropout=dropout)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (Transformer)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: Transformer (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

transformer_model, transformer_history = train_and_evaluate_transformer(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""## Yet Another feature combination"""





import numpy as np
from sklearn.preprocessing import StandardScaler

# Load splits
data = np.load("emg_splits.npz")
X_train, y_train = data["X_train"], data["y_train"]
X_val, y_val     = data["X_val"], data["y_val"]
X_test, y_test   = data["X_test"], data["y_test"]

# Define feature names and index ranges
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

# Each feature occupies 8 consecutive columns
feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

# Function to extract selected feature subsets
def get_feature_subset(X, selected_features):
    indices = []
    for f in selected_features:
        indices.extend(feature_indices[f])
    return X[:, indices]

# Function to normalize train/val/test using train stats
def normalize_datasets(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled   = scaler.transform(X_val)
    X_test_scaled  = scaler.transform(X_test)
    return X_train_scaled, X_val_scaled, X_test_scaled



selected = ["zero_cross", "aac", "wfl"]

X_train_sub = get_feature_subset(X_train, selected)
X_val_sub   = get_feature_subset(X_val, selected)
X_test_sub  = get_feature_subset(X_test, selected)

X_train_sub_norm, X_val_sub_norm, X_test_sub_norm = normalize_datasets(
    X_train_sub, X_val_sub, X_test_sub
)

X_train_sub_norm.shape



"""### ML MODEL FITTING

#### LR
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay,accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val for GridSearch
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],      # Regularization strength
    'solver': ['liblinear', 'lbfgs'], # Solvers that work for small/mid-size data
    'penalty': ['l2']                 # Only 'l2' works with lbfgs/liblinear
}

# Grid search
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train_val, y_train_val)

# Best model
best_model = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_model.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Logistic Regression (Best Model)")
plt.grid(False)
plt.show()

"""#### RF"""



from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm,))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_rf = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_rf.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: Random Forest (Best Model)")
plt.grid(False)
plt.show()

"""#### SVM"""



from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm, X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid for RBF kernel
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

# Grid search
grid = GridSearchCV(
    SVC(probability=True),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_svm = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_svm.predict(X_test_sub_norm)

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: SVM (Best Model)")
plt.grid(False)
plt.show()

"""#### KNN"""



from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Merge train + val
X_train_val = np.vstack((X_train_sub_norm,X_val_sub_norm))
y_train_val = np.concatenate((y_train, y_val))

# Hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski']
}

# Grid search
grid = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train_val, y_train_val)

# Best model
best_knn = grid.best_estimator_
print("Best hyperparameters:", grid.best_params_)

# Evaluate on test set
y_test_pred = best_knn.predict(X_test_sub_norm )

# Accuracy and report
print("\nTest Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix: KNN (Best Model)")
plt.grid(False)
plt.show()





"""#### MLP"""



import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based for Keras categorical
num_classes = 7
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

# Optional: One-hot encoding if needed (for categorical_crossentropy)
# y_train_oh = tf.keras.utils.to_categorical(y_train_keras, num_classes)
# y_val_oh   = tf.keras.utils.to_categorical(y_val_keras, num_classes)
# y_test_oh  = tf.keras.utils.to_categorical(y_test_keras, num_classes)

def build_mlp(input_dim, hidden_layers=[128, 64], dropout_rate=0.3, output_dim=7):
    model = Sequential()
    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(input_dim,)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    for units in hidden_layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    model.add(Dense(output_dim, activation='softmax'))
    return model

def train_and_evaluate_mlp(hidden_layers=[128, 64], dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_mlp(input_dim=24, hidden_layers=hidden_layers, dropout_rate=dropout_rate)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_sub_norm, y_train_keras,
        validation_data=(X_val_sub_norm, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test
    test_loss, test_acc = model.evaluate( X_test_sub_norm, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_sub_norm).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    # Confusion matrix
    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: MLP")
    plt.grid(False)
    plt.show()

    return model, history

# Example config
model, history = train_and_evaluate_mlp(
    hidden_layers=[128, 64],
    dropout_rate=0.4,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### 2D CNN"""



# Reshape feature vectors into 2D "images" of shape (8, 10, 1)
def reshape_to_2d(X):
    return X.reshape((-1, 8, 3, 1))  # (samples, height, width, channels)

X_train_2d = reshape_to_2d(X_train_sub_norm)
X_val_2d   = reshape_to_2d(X_val_sub_norm)
X_test_2d  = reshape_to_2d(X_test_sub_norm)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Convert labels to 0-based
y_train_keras = y_train - 1
y_val_keras = y_val - 1
y_test_keras = y_test - 1

def build_cnn_model(input_shape=(8, 3, 1), dropout_rate=0.3, num_classes=7):
    model = Sequential([
        tf.keras.Input(shape=input_shape),

        Conv2D(32, kernel_size=(3, 1), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 1)),  # Only pool over height
        Dropout(dropout_rate),

        Conv2D(64, kernel_size=(2, 1), activation='relu'),
        BatchNormalization(),
        Flatten(),

        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(num_classes, activation='softmax')
    ])
    return model


def train_and_evaluate_cnn(dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    model = build_cnn_model(dropout_rate=dropout_rate)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train_2d, y_train_keras,
        validation_data=(X_val_2d, y_val_keras),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(X_test_2d, y_test_keras, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test_2d).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test_keras, y_pred, digits=4))

    cm = confusion_matrix(y_test_keras, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5,6,7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: 2D CNN")
    plt.grid(False)
    plt.show()

    return model, history

cnn_model, cnn_history = train_and_evaluate_cnn(
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### sequential modelling"""



import numpy as np

def build_feature_sequences(X, y, time_steps=5):
    sequences = []
    labels = []
    for i in range(len(X) - time_steps + 1):
        window = X[i:i+time_steps]
        window_labels = y[i:i+time_steps]

        # Only keep sequences where all labels are the same
        if np.all(window_labels == window_labels[0]):
            sequences.append(window)
            labels.append(window_labels[0])

    return np.array(sequences), np.array(labels)

import pandas as pd
import numpy as np

# Load original dataset
df = pd.read_csv("/content/emg_all_features_labeled.csv", header=None)
X_full = df.iloc[:, :-1].values  # all features
y_full = df.iloc[:, -1].values   # labels

# Define all feature names and extract indices for selection
feature_names = [
    "std", "rms", "min", "max", "zero_cross", "aac",
    "afb", "mav", "wfl", "wamp"
]

feature_indices = {
    name: list(range(i * 8, (i + 1) * 8)) for i, name in enumerate(feature_names)
}

selected = ["zero_cross", "aac", "wfl"]
selected_indices = []
for f in selected:
    selected_indices.extend(feature_indices[f])

# Subset feature matrix
X_selected = X_full[:, selected_indices]

# Build sequences (assume build_feature_sequences is already defined)
X_seq, y_seq = build_feature_sequences(X_selected, y_full, time_steps=5)

from sklearn.preprocessing import StandardScaler

num_sequences, time_steps, num_features = X_seq.shape

# Flatten → scale → reshape back
X_seq_flat = X_seq.reshape(-1, num_features)
X_seq_scaled_flat = StandardScaler().fit_transform(X_seq_flat)
X_seq_scaled = X_seq_scaled_flat.reshape(num_sequences, time_steps, num_features)

from sklearn.model_selection import train_test_split

X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(
    X_seq_scaled, y_seq, test_size=0.30, random_state=42, stratify=y_seq
)

X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(
    X_temp_seq, y_temp_seq, test_size=0.50, random_state=42, stratify=y_temp_seq
)

print("Train seq shape:", X_train_seq.shape)
print("Val seq shape:  ", X_val_seq.shape)
print("Test seq shape: ", X_test_seq.shape)

"""#### LSTM"""



# Convert labels to 0-based (0 to 6 for 7 classes)
y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def build_lstm_model(input_shape, dropout_rate=0.3, num_classes=7):
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(LSTM(32))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))

    return model

def train_and_evaluate_lstm(X_train, y_train, X_val, y_val, X_test, y_test,
                            dropout_rate=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_lstm_model(input_shape, dropout_rate=dropout_rate)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (LSTM)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Test evaluation
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: LSTM (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

lstm_model, lstm_history = train_and_evaluate_lstm(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout_rate=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""#### transformer"""



y_train_seq_keras = y_train_seq - 1
y_val_seq_keras   = y_val_seq - 1
y_test_seq_keras  = y_test_seq - 1

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Transformer encoder block
def transformer_encoder(inputs, head_size=64, num_heads=2, ff_dim=128, dropout=0.3):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)

    ff = layers.Dense(ff_dim, activation='relu')(x)
    ff = layers.Dropout(dropout)(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# Build the Transformer model
def build_transformer_model(input_shape, dropout=0.3, num_classes=7):
    inputs = layers.Input(shape=input_shape)

    # Project input to model dimension (e.g., 128) for attention to work
    x = layers.Dense(128)(inputs)

    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=dropout)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

def train_and_evaluate_transformer(X_train, y_train, X_val, y_val, X_test, y_test,
                                   dropout=0.3, learning_rate=0.001, batch_size=64, epochs=100):
    input_shape = (X_train.shape[1], X_train.shape[2])  # (5, 24)
    model = build_transformer_model(input_shape=input_shape, dropout=dropout)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        verbose=1
    )

    # Plot training vs validation loss
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss (Transformer)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluate
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nTest Accuracy: {test_acc:.4f}")

    y_pred = model.predict(X_test).argmax(axis=1)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 2, 3, 4, 5, 6, 7])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix: Transformer (Selected Features)")
    plt.grid(False)
    plt.show()

    return model, history

transformer_model, transformer_history = train_and_evaluate_transformer(
    X_train_seq, y_train_seq_keras,
    X_val_seq, y_val_seq_keras,
    X_test_seq, y_test_seq_keras,
    dropout=0.3,
    learning_rate=0.001,
    batch_size=64,
    epochs=100
)

"""# OVERALL RESULT SUMMARY

| **Model**      | **All Features** | `["rms", "mav", "wamp"]` | `["std", "min", "max"]` | `["zero_cross", "aac", "afb"]` | `["rms", "wfl", "wamp"]` | `["zero_cross", "aac", "wfl"]` | **Avg Accuracy** |
|----------------|------------------|--------------------------|--------------------------|-------------------------------|--------------------------|-------------------------------|------------------|
| **LogReg**     | 0.9033  (C=100, L2, solver=lbfgs)         | 0.8613  (same)          | 0.8320  (same)          | 0.8375  (same)             | 0.8398  (same)          | 0.8095  (same)             | **0.8472**       |
| **RandomForest** | 0.9599  (min_samples_split=2, n=200)      | 0.9648  (min_samples_split=2, n=200) | 0.9472 (same)       | 0.9570 (n=200)             | 0.9649  (same)          | 0.9589 (same)             | **0.9588**       |
| **SVM**        | 0.9609  (C=10, gamma=scale, rbf)           | 0.9334  (same)          | 0.9335  (C=10, lbfgs)   | 0.9472  (C=10, rbf)         | 0.9326  (C=10, rbf)      | 0.9453  (C=10, rbf)        | **0.9422**       |
| **KNN**        | 0.9691  (minkowski, n=3)                   | 0.9518  (minkowski, n=5) | 0.9218  (same)          | 0.9375  (same)             | 0.9248  (same)          | 0.9453  (same)             | **0.9417**       |
| **MLP**        | 0.9375  (128-64, dropout=0.3, lr=0.001)    | 0.9414  (same)          | 0.9355  (same)          | 0.9531  (same)             | 0.9424  (same)          | 0.9472  (same)             | **0.9428**       |
| **2D CNN**     | 0.9570  (2-layer, 32→64 filters, dense=128) | 0.9580  (same)          | 0.9326  (same)          | 0.9453  (same)             | 0.9541  (same)          | 0.9500  (same)             | **0.9495**       |
| **LSTM**       | 0.9804  (2-layer LSTM + dense)             | 0.9788  (same)          | 0.9150  (same)          | 0.9590  (same)             | 0.9689  (same)          | 0.9579  (same)             | **0.9600**       |
| **Transformer**| 0.9812  (attn_dim=128, heads=2, ff_dim=128) | 0.9889  (same)          | 0.9370  (same)          | 0.9522  (same)             | 0.9618  (same)          | 0.9698  (same)             | **0.9652**       |

🧠 Notes on Configuration
Logistic Regression: C=100, penalty=L2, solver=lbfgs used in all

Random Forest: consistently used min_samples_split=2, n_estimators=200

SVM: RBF kernel with C=10, gamma=scale in most cases

KNN: distance metric = minkowski; tried k=3 and k=5

MLP: 2 hidden layers [128, 64], dropout=0.3, learning_rate=0.001

2D CNN: Two conv layers (32→64), followed by dense 128

LSTM: Sequential 2-layer LSTM + 1 dense layer

Transformer: attention_dim=128, heads=2, feedforward=128

📊 Observations
Transformer achieved the highest average accuracy: 0.9652

LSTM is very close behind: 0.9600

Among classical models, Random Forest is best: 0.9588

LogReg underperforms compared to all other models
"""





"""#### Creating heatmap of the result table"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Data for heatmap (accuracy values)
data = {
    "All Features":           [0.9033, 0.9599, 0.9609, 0.9691, 0.9375, 0.9570, 0.9804, 0.9812],
    "['rms', 'mav', 'wamp']": [0.8613, 0.9648, 0.9334, 0.9518, 0.9414, 0.9580, 0.9788, 0.9889],
    "['std', 'min', 'max']":  [0.8320, 0.9472, 0.9335, 0.9218, 0.9355, 0.9326, 0.9150, 0.9370],
    "['zero, aac, afb']":     [0.8375, 0.9570, 0.9472, 0.9375, 0.9531, 0.9453, 0.9590, 0.9522],
    "['rms', 'wfl', 'wamp']": [0.8398, 0.9649, 0.9326, 0.9248, 0.9424, 0.9541, 0.9689, 0.9618],
    "['zero, aac, wfl']":     [0.8095, 0.9589, 0.9453, 0.9453, 0.9472, 0.9500, 0.9579, 0.9698],
}

# Row labels
models = ["LogReg", "RF", "SVM", "KNN", "MLP", "2D CNN", "LSTM", "Transformer"]

# Create DataFrame
df = pd.DataFrame(data, index=models)

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df, annot=True, fmt=".4f", cmap="YlGnBu", linewidths=0.5, cbar_kws={'label': 'Accuracy'})

plt.title("Classification Accuracy Heatmap: Models vs Feature Sets", fontsize=14)
plt.xlabel("Feature Combination")
plt.ylabel("Model")
plt.tight_layout()
plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv("/content/drive/MyDrive/mlsp dataset/emg_all_features_labeled.csv", header=None)
X = df.iloc[:, :-1].values  # shape: (N, 80)
y = df.iloc[:, -1].values   # shape: (N,)

# Feature group names
feature_groups = [
    "std", "rms", "min", "max", "zero_cross",
    "aac", "afb", "mav", "wfl", "wamp"
]

# Select a label to visualize (e.g., 5 = thumb)
gesture_label = 6
n_timesteps = 50

# Get the starting index of the gesture
start_idx = np.where(y == gesture_label)[0][0]
X_seq = X[start_idx:start_idx + n_timesteps]

# Generate labels per feature column
feature_labels = []
for feature in feature_groups:
    feature_labels.extend([f"{feature}_e{i+1}" for i in range(8)])

# Line plot of selected features
selected_feature_names = ["rms_e1", "rms_e2", "mav_e1", "mav_e2", "wamp_e1"]
selected_indices = [feature_labels.index(f) for f in selected_feature_names]

plt.figure(figsize=(8, 4))
for idx in selected_indices:
    plt.plot(X_seq[:, idx], label=feature_labels[idx])
plt.title(f"Temporal Evolution of Features for Label {gesture_label}")
plt.xlabel("Time Step")
plt.ylabel("Feature Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Heatmap of all 80 features
plt.figure(figsize=(12, 6))
sns.heatmap(X_seq, cmap='viridis', xticklabels=feature_labels, yticklabels=True)
plt.title(f"Heatmap of Feature Matrix Over Time (Label {gesture_label})")
plt.xlabel("Feature")
plt.ylabel("Time Step")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()



import pandas as pd

# Load the CSV file (no header assumed)
df = pd.read_csv("/content/drive/MyDrive/mlsp dataset/little_finger_motion_raw.csv", header=None)

# Count rows
num_rows = df.shape[0]
num_columns = df.shape[1]

print(f"Total rows (samples): {num_rows}")
print(f"Total columns (features/electrodes): {num_columns}")

import pandas as pd

# Load the CSV (assuming no header, label is last column)
df = pd.read_csv("/content/drive/MyDrive/mlsp dataset/emg_all_features_labeled.csv", header=None)

# Extract labels (last column)
labels = df.iloc[:, -1]

# Count how many rows have label == 1
label_1_count = (labels == 2).sum()

print(f"Number of rows with label 1: {label_1_count}")

